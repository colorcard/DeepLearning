# å¯¼å…¥ PyTorch å’Œç›¸å…³æ¨¡å—
import torch
from torch import nn                            # ç¥ç»ç½‘ç»œæ¨¡å—
from torchvision import datasets, transforms    # å¸¸ç”¨æ•°æ®é›†å’Œé¢„å¤„ç†
from torch.utils.data import DataLoader         # æ‰¹é‡æ•°æ®åŠ è½½å™¨
import matplotlib.pyplot as plt                 # ğŸ¨ ç”¨äºç»˜å›¾ï¼ˆæ–°å¢ï¼‰

# ========================
# ğŸ§© é…ç½®åŒºåŸŸï¼ˆå¯å¿«é€Ÿè°ƒæ•´ï¼‰
# ========================
BATCH_SIZE = 64             # æ¯æ‰¹è®­ç»ƒæ•°æ®çš„å¤§å°
TEST_BATCH_SIZE = 1000      # æ¯æ‰¹æµ‹è¯•æ•°æ®çš„å¤§å°
HIDDEN_SIZE = 128           # éšè—å±‚ç¥ç»å…ƒæ•°é‡ï¼ˆMLP ä¸­é—´å±‚ç»´åº¦ï¼‰
LEARNING_RATE = 1e-3        # å­¦ä¹ ç‡ï¼ˆæ§åˆ¶æ¨¡å‹æ›´æ–°é€Ÿåº¦ï¼‰
EPOCHS = 6                  # æ€»è®­ç»ƒè½®æ•°
DATA_PATH = "../data"       # MNIST æ•°æ®ä¿å­˜è·¯å¾„
MODEL_PATH = "../pth/mnist_mlp.pth"  # æ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„
SAVE = True             # æ˜¯å¦ä¿å­˜æ¨¡å‹å‚æ•°

# è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼šä¼˜å…ˆä½¿ç”¨ M1/M2/M3 èŠ¯ç‰‡çš„ MPS åŠ é€Ÿï¼ˆMacï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ CPU
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")  # æ‰“å°ä½¿ç”¨çš„è®¾å¤‡ç±»å‹

# å®šä¹‰æ•°æ®é¢„å¤„ç†æ–¹å¼ï¼šå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ï¼ˆTensorï¼‰ï¼Œå€¼ç¼©æ”¾åˆ° 0~1
transform = transforms.ToTensor()

# åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼ˆMNIST æ‰‹å†™æ•°å­—å›¾ç‰‡ï¼‰
train_data = datasets.MNIST(
    root=DATA_PATH,        # ä¸‹è½½è·¯å¾„
    train=True,            # è®­ç»ƒé›†
    download=True,         # å¦‚æœæ²¡æœ‰å°±è‡ªåŠ¨ä¸‹è½½
    transform=transform    # åº”ç”¨é¢„å¤„ç†
)

# åŠ è½½æµ‹è¯•æ•°æ®é›†
test_data = datasets.MNIST(
    root=DATA_PATH,
    train=False,           # æµ‹è¯•é›†
    download=True,
    transform=transform
)

# æ„é€ è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒæ‰¹é‡è¯»å–å¹¶æ‰“ä¹±é¡ºåº
train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)

# æ„é€ æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œä¸æ‰“ä¹±é¡ºåº
test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE)

# æ„å»ºä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆMLPï¼‰
model = nn.Sequential(
    nn.Flatten(),                      # å°† 28x28 å›¾åƒå±•å¹³ä¸º 784 å‘é‡
    nn.Linear(28 * 28, HIDDEN_SIZE),   # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼šè¾“å…¥784ç»´ï¼Œè¾“å‡ºéšè—å±‚ç»´åº¦
    nn.ReLU(),                         # æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
    nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),   # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼šè¾“å…¥784ç»´ï¼Œè¾“å‡ºéšè—å±‚ç»´åº¦
    nn.ReLU(),                         # æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
    nn.Linear(HIDDEN_SIZE, 10)         # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼šè¾“å‡º10ç±»ï¼ˆæ•°å­—0~9ï¼‰
).to(device)                           # æŠŠæ¨¡å‹æ”¾åˆ°æŒ‡å®šè®¾å¤‡ä¸Šï¼ˆMPS æˆ– CPUï¼‰

# å®šä¹‰æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆåˆ†ç±»é—®é¢˜çš„å¸¸ç”¨é€‰æ‹©ï¼‰
loss_fn = nn.CrossEntropyLoss()

# å®šä¹‰ä¼˜åŒ–å™¨ï¼šAdamï¼ˆè‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡çš„ä¼˜åŒ–ç®—æ³•ï¼‰
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# ğŸ” ç”¨äºå¯è§†åŒ–ï¼šè®°å½•æ¯è½®çš„è®­ç»ƒ/æµ‹è¯•æŸå¤±ä¸å‡†ç¡®ç‡
train_losses = []
test_losses = []
test_accuracies = []

# å®šä¹‰è®­ç»ƒè¿‡ç¨‹ï¼ˆæ‰§è¡Œä¸€æ¬¡å®Œæ•´ epochï¼‰
def train(epoch):
    model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    running_loss = 0  # å½“å‰ epoch çš„æ€»æŸå¤±
    for batch, (X, y) in enumerate(train_loader):  # éå†æ¯ä¸€æ‰¹æ•°æ®
        X, y = X.to(device), y.to(device)  # å°†æ•°æ®ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡

        pred = model(X)             # å‰å‘ä¼ æ’­ï¼šè·å¾—é¢„æµ‹ç»“æœ
        loss = loss_fn(pred, y)     # è®¡ç®—æŸå¤±å€¼ï¼ˆé¢„æµ‹ vs çœŸå€¼ï¼‰

        optimizer.zero_grad()       # æ¸…é™¤ä¸Šä¸€æ­¥æ®‹ç•™çš„æ¢¯åº¦
        loss.backward()             # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        optimizer.step()            # æ‰§è¡Œå‚æ•°æ›´æ–°ï¼ˆä¼˜åŒ–ï¼‰

        running_loss += loss.item()

        if batch % 100 == 0:  # æ¯éš”100ä¸ª batch æ‰“å°ä¸€æ¬¡å½“å‰æŸå¤±
            print(f"Epoch {epoch} [{batch * len(X)}/{len(train_loader.dataset)}] Loss: {loss.item():.4f}")

    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)  # ğŸ“ˆ è®°å½•è®­ç»ƒæŸå¤±

# å®šä¹‰æµ‹è¯•è¿‡ç¨‹ï¼ˆä¸ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ï¼‰
def test():
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ Dropout ç­‰è®­ç»ƒç‰¹æ€§ï¼‰
    correct = 0                # æ­£ç¡®é¢„æµ‹æ•°
    total = 0                  # æ€»æ ·æœ¬æ•°
    total_loss = 0             # ç´¯è®¡æŸå¤±
    with torch.no_grad():     # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆèŠ‚çœèµ„æºï¼‰
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            output = model(X)                           # å‰å‘ä¼ æ’­ï¼Œé¢„æµ‹ç»“æœ
            loss = loss_fn(output, y)                   # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æŸå¤±
            total_loss += loss.item() * y.size(0)       # ä¹˜ä¸Šæ ·æœ¬æ•°é‡ï¼ŒåŠ æƒç´¯ç§¯
            pred = output.argmax(1)                     # è·å–æ¯ä¸ªæ ·æœ¬é¢„æµ‹çš„ç±»åˆ«ï¼ˆæœ€å¤§å€¼ä½ç½®ï¼‰
            correct += (pred == y).sum().item()         # ç´¯åŠ é¢„æµ‹æ­£ç¡®çš„æ•°é‡
            total += y.size(0)                          # ç´¯åŠ æ ·æœ¬æ•°é‡

    avg_loss = total_loss / total     # è®¡ç®—å¹³å‡æŸå¤±
    acc = 100. * correct / total      # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    print(f"Test Accuracy: {acc:.2f}%, Test Loss: {avg_loss:.4f}\n")

    test_losses.append(avg_loss)         # ğŸ“‰ è®°å½•æµ‹è¯•æŸå¤±
    test_accuracies.append(acc)          # âœ… è®°å½•å‡†ç¡®ç‡

# è®­ç»ƒä¸»å¾ªç¯ï¼šé‡å¤æ‰§è¡Œè®­ç»ƒ + æµ‹è¯•
for epoch in range(1, EPOCHS + 1):
    train(epoch)  # è®­ç»ƒä¸€è½®
    test()        # æµ‹è¯•ä¸€è½®

# ä¿å­˜æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶ï¼ˆå¦‚ï¼šmnist_mlp.pthï¼‰
if SAVE:
    torch.save(model.state_dict(), MODEL_PATH)
    print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º " + MODEL_PATH)

# ğŸ”§ å¯è§†åŒ–ï¼šè®­ç»ƒ & æµ‹è¯•è¿‡ç¨‹
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label="Train Loss")
plt.plot(test_losses, label="Test Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(test_accuracies, label="Test Accuracy", color="green")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy Curve")
plt.legend()

plt.tight_layout()
plt.show()